{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\huge{\\text{  Module 3 GridSearch and Pipelines}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:20:51.293146Z",
     "start_time": "2023-02-02T17:20:50.182691Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T15:39:03.321551Z",
     "start_time": "2020-11-16T15:39:02.272983Z"
    }
   },
   "source": [
    "$ \\huge{\\text{ Overview}}$\n",
    "\n",
    "<font size =4> Our goal in the section is to run a number of models at the same time and find out what processes and parameters produce the best models.  In our first example we will run 24 models at the same time.  We will introduce a few new procedures __PCA__ and __Neural Networks__ that I will very briefly describe but go into greater detail in __Module 4 and Module 5.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "<font size=4>\n",
    "    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html \n",
    "    \n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to ‘passthrough’ or None.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch \n",
    "<font size=4>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=grid%20searchcv\n",
    "\n",
    "Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "Important members are fit, predict.\n",
    "\n",
    "GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "\n",
    "<font size=4>Let's look at  SKlearns's  PCA and Logistic Regression, specifically let's look at all the parameters.\n",
    "    \n",
    "<font color=blue>__[PCA Parameters](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)__</font>\n",
    "    \n",
    "<dt><strong>n_components</strong><span class=\"classifier\">int, float or ‘mle’, default=None</span></dt><dd><p>Number of components to keep.\n",
    "if n_components is not set all components are kept:</p>\n",
    "<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">n_components</span> <span class=\"o\">==</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">n_features</span><span class=\"p\">)</span>\n",
    "</pre></div>\n",
    "</div>\n",
    "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">n_components</span> <span class=\"pre\">==</span> <span class=\"pre\">'mle'</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">svd_solver</span> <span class=\"pre\">==</span> <span class=\"pre\">'full'</span></code>, Minka’s\n",
    "MLE is used to guess the dimension. Use of <code class=\"docutils literal notranslate\"><span class=\"pre\">n_components</span> <span class=\"pre\">==</span> <span class=\"pre\">'mle'</span></code>\n",
    "will interpret <code class=\"docutils literal notranslate\"><span class=\"pre\">svd_solver</span> <span class=\"pre\">==</span> <span class=\"pre\">'auto'</span></code> as <code class=\"docutils literal notranslate\"><span class=\"pre\">svd_solver</span> <span class=\"pre\">==</span> <span class=\"pre\">'full'</span></code>.</p>\n",
    "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">&lt;</span> <span class=\"pre\">n_components</span> <span class=\"pre\">&lt;</span> <span class=\"pre\">1</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">svd_solver</span> <span class=\"pre\">==</span> <span class=\"pre\">'full'</span></code>, select the\n",
    "number of components such that the amount of variance that needs to be\n",
    "explained is greater than the percentage specified by n_components.</p>\n",
    "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">svd_solver</span> <span class=\"pre\">==</span> <span class=\"pre\">'arpack'</span></code>, the number of components must be\n",
    "strictly less than the minimum of n_features and n_samples.</p>\n",
    "<p>Hence, the None case results in:</p>\n",
    "<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">n_components</span> <span class=\"o\">==</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">n_features</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n",
    "</pre></div>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>copy</strong><span class=\"classifier\">bool, default=True</span></dt><dd><p>If False, data passed to fit are overwritten and running\n",
    "fit(X).transform(X) will not yield the expected results,\n",
    "use fit_transform(X) instead.</p>\n",
    "</dd>\n",
    "<dt><strong>whiten</strong><span class=\"classifier\">bool, default=False</span></dt><dd><p>When True (False by default) the <code class=\"docutils literal notranslate\"><span class=\"pre\">components_</span></code> vectors are multiplied\n",
    "by the square root of n_samples and then divided by the singular values\n",
    "to ensure uncorrelated outputs with unit component-wise variances.</p>\n",
    "<p>Whitening will remove some information from the transformed signal\n",
    "(the relative variance scales of the components) but can sometime\n",
    "improve the predictive accuracy of the downstream estimators by\n",
    "making their data respect some hard-wired assumptions.</p>\n",
    "</dd>\n",
    "<dt><strong>svd_solver</strong><span class=\"classifier\">{‘auto’, ‘full’, ‘arpack’, ‘randomized’}, default=’auto’</span></dt><dd><dl class=\"simple\">\n",
    "<dt>If auto :</dt><dd><p>The solver is selected by a default policy based on <code class=\"docutils literal notranslate\"><span class=\"pre\">X.shape</span></code> and\n",
    "<code class=\"docutils literal notranslate\"><span class=\"pre\">n_components</span></code>: if the input data is larger than 500x500 and the\n",
    "number of components to extract is lower than 80% of the smallest\n",
    "dimension of the data, then the more efficient ‘randomized’\n",
    "method is enabled. Otherwise the exact full SVD is computed and\n",
    "optionally truncated afterwards.</p>\n",
    "</dd>\n",
    "<dt>If full :</dt><dd><p>run exact full SVD calling the standard LAPACK solver via\n",
    "<code class=\"docutils literal notranslate\"><span class=\"pre\">scipy.linalg.svd</span></code> and select the components by postprocessing</p>\n",
    "</dd>\n",
    "<dt>If arpack :</dt><dd><p>run SVD truncated to n_components calling ARPACK solver via\n",
    "<code class=\"docutils literal notranslate\"><span class=\"pre\">scipy.sparse.linalg.svds</span></code>. It requires strictly\n",
    "0 &lt; n_components &lt; min(X.shape)</p>\n",
    "</dd>\n",
    "<dt>If randomized :</dt><dd><p>run randomized SVD by the method of Halko et al.</p>\n",
    "</dd>\n",
    "</dl>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.18.0.</span></p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>tol</strong><span class=\"classifier\">float, default=0.0</span></dt><dd><p>Tolerance for singular values computed by svd_solver == ‘arpack’.\n",
    "Must be of range [0.0, infinity).</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.18.0.</span></p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>iterated_power</strong><span class=\"classifier\">int or ‘auto’, default=’auto’</span></dt><dd><p>Number of iterations for the power method computed by\n",
    "svd_solver == ‘randomized’.\n",
    "Must be of range [0, infinity).</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.18.0.</span></p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>random_state</strong><span class=\"classifier\">int, RandomState instance or None, default=None</span></dt><dd><p>Used when the ‘arpack’ or ‘randomized’ solvers are used. Pass an int\n",
    "for reproducible results across multiple function calls.\n",
    "See <a class=\"reference internal\" href=\"../../glossary.html#term-random_state\"><span class=\"xref std std-term\">Glossary</span></a>.</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.18.0.</span></p>\n",
    "</div>\n",
    "</dd>\n",
    "</dl>\n",
    "</dd>\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "<font color=blue>__[Logisitic Regression Parameters](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)__</font>\n",
    "    \n",
    "<dd class=\"field-odd\"><dl>\n",
    "<dt><strong>penalty</strong><span class=\"classifier\">{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’</span></dt><dd><p>Used to specify the norm used in the penalization. The ‘newton-cg’,\n",
    "‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is\n",
    "only supported by the ‘saga’ solver. If ‘none’ (not supported by the\n",
    "liblinear solver), no regularization is applied.</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.19: </span>l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)</p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>dual</strong><span class=\"classifier\">bool, default=False</span></dt><dd><p>Dual or primal formulation. Dual formulation is only implemented for\n",
    "l2 penalty with liblinear solver. Prefer dual=False when\n",
    "n_samples &gt; n_features.</p>\n",
    "</dd>\n",
    "<dt><strong>tol</strong><span class=\"classifier\">float, default=1e-4</span></dt><dd><p>Tolerance for stopping criteria.</p>\n",
    "</dd>\n",
    "<dt><strong>C</strong><span class=\"classifier\">float, default=1.0</span></dt><dd><p>Inverse of regularization strength; must be a positive float.\n",
    "Like in support vector machines, smaller values specify stronger\n",
    "regularization.</p>\n",
    "</dd>\n",
    "<dt><strong>fit_intercept</strong><span class=\"classifier\">bool, default=True</span></dt><dd><p>Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "added to the decision function.</p>\n",
    "</dd>\n",
    "<dt><strong>intercept_scaling</strong><span class=\"classifier\">float, default=1</span></dt><dd><p>Useful only when the solver ‘liblinear’ is used\n",
    "and self.fit_intercept is set to True. In this case, x becomes\n",
    "[x, self.intercept_scaling],\n",
    "i.e. a “synthetic” feature with constant value equal to\n",
    "intercept_scaling is appended to the instance vector.\n",
    "The intercept becomes <code class=\"docutils literal notranslate\"><span class=\"pre\">intercept_scaling</span> <span class=\"pre\">*</span> <span class=\"pre\">synthetic_feature_weight</span></code>.</p>\n",
    "<p>Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "as all other features.\n",
    "To lessen the effect of regularization on synthetic feature weight\n",
    "(and therefore on the intercept) intercept_scaling has to be increased.</p>\n",
    "</dd>\n",
    "<dt><strong>class_weight</strong><span class=\"classifier\">dict or ‘balanced’, default=None</span></dt><dd><p>Weights associated with classes in the form <code class=\"docutils literal notranslate\"><span class=\"pre\">{class_label:</span> <span class=\"pre\">weight}</span></code>.\n",
    "If not given, all classes are supposed to have weight one.</p>\n",
    "<p>The “balanced” mode uses the values of y to automatically adjust\n",
    "weights inversely proportional to class frequencies in the input data\n",
    "as <code class=\"docutils literal notranslate\"><span class=\"pre\">n_samples</span> <span class=\"pre\">/</span> <span class=\"pre\">(n_classes</span> <span class=\"pre\">*</span> <span class=\"pre\">np.bincount(y))</span></code>.</p>\n",
    "<p>Note that these weights will be multiplied with sample_weight (passed\n",
    "through the fit method) if sample_weight is specified.</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.17: </span><em>class_weight=’balanced’</em></p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>random_state</strong><span class=\"classifier\">int, RandomState instance, default=None</span></dt><dd><p>Used when <code class=\"docutils literal notranslate\"><span class=\"pre\">solver</span></code> == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the\n",
    "data. See <a class=\"reference internal\" href=\"../../glossary.html#term-random-state\"><span class=\"xref std std-term\">Glossary</span></a> for details.</p>\n",
    "</dd>\n",
    "<dt><strong>solver</strong><span class=\"classifier\">{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},             default=’lbfgs’</span></dt><dd><p>Algorithm to use in the optimization problem.</p>\n",
    "<ul class=\"simple\">\n",
    "<li><p>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and\n",
    "‘saga’ are faster for large ones.</p></li>\n",
    "<li><p>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’\n",
    "handle multinomial loss; ‘liblinear’ is limited to one-versus-rest\n",
    "schemes.</p></li>\n",
    "<li><p>‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty</p></li>\n",
    "<li><p>‘liblinear’ and ‘saga’ also handle L1 penalty</p></li>\n",
    "<li><p>‘saga’ also supports ‘elasticnet’ penalty</p></li>\n",
    "<li><p>‘liblinear’ does not support setting <code class=\"docutils literal notranslate\"><span class=\"pre\">penalty='none'</span></code></p></li>\n",
    "</ul>\n",
    "<p>Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on\n",
    "features with approximately the same scale. You can\n",
    "preprocess the data with a scaler from sklearn.preprocessing.</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>\n",
    "</div>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.19: </span>SAGA solver.</p>\n",
    "</div>\n",
    "<div class=\"versionchanged\">\n",
    "<p><span class=\"versionmodified changed\">Changed in version 0.22: </span>The default solver changed from ‘liblinear’ to ‘lbfgs’ in 0.22.</p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>max_iter</strong><span class=\"classifier\">int, default=100</span></dt><dd><p>Maximum number of iterations taken for the solvers to converge.</p>\n",
    "</dd>\n",
    "<dt><strong>multi_class</strong><span class=\"classifier\">{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’</span></dt><dd><p>If the option chosen is ‘ovr’, then a binary problem is fit for each\n",
    "label. For ‘multinomial’ the loss minimised is the multinomial loss fit\n",
    "across the entire probability distribution, <em>even when the data is\n",
    "binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’.\n",
    "‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,\n",
    "and otherwise selects ‘multinomial’.</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p>\n",
    "</div>\n",
    "<div class=\"versionchanged\">\n",
    "<p><span class=\"versionmodified changed\">Changed in version 0.22: </span>Default changed from ‘ovr’ to ‘auto’ in 0.22.</p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>verbose</strong><span class=\"classifier\">int, default=0</span></dt><dd><p>For the liblinear and lbfgs solvers set verbose to any positive\n",
    "number for verbosity.</p>\n",
    "</dd>\n",
    "<dt><strong>warm_start</strong><span class=\"classifier\">bool, default=False</span></dt><dd><p>When set to True, reuse the solution of the previous call to fit as\n",
    "initialization, otherwise, just erase the previous solution.\n",
    "Useless for liblinear solver. See <a class=\"reference internal\" href=\"../../glossary.html#term-warm-start\"><span class=\"xref std std-term\">the Glossary</span></a>.</p>\n",
    "<div class=\"versionadded\">\n",
    "<p><span class=\"versionmodified added\">New in version 0.17: </span><em>warm_start</em> to support <em>lbfgs</em>, <em>newton-cg</em>, <em>sag</em>, <em>saga</em> solvers.</p>\n",
    "</div>\n",
    "</dd>\n",
    "<dt><strong>n_jobs</strong><span class=\"classifier\">int, default=None</span></dt><dd><p>Number of CPU cores used when parallelizing over classes if\n",
    "multi_class=’ovr’”. This parameter is ignored when the <code class=\"docutils literal notranslate\"><span class=\"pre\">solver</span></code> is\n",
    "set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or\n",
    "not. <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> means 1 unless in a <a class=\"reference external\" href=\"https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend\" title=\"(in joblib v0.17.0.dev0)\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">joblib.parallel_backend</span></code></a>\n",
    "context. <code class=\"docutils literal notranslate\"><span class=\"pre\">-1</span></code> means using all processors.\n",
    "See <a class=\"reference internal\" href=\"../../glossary.html#term-n-jobs\"><span class=\"xref std std-term\">Glossary</span></a> for more details.</p>\n",
    "</dd>\n",
    "<dt><strong>l1_ratio</strong><span class=\"classifier\">float, default=None</span></dt><dd><p>The Elastic-Net mixing parameter, with <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">&lt;=</span> <span class=\"pre\">l1_ratio</span> <span class=\"pre\">&lt;=</span> <span class=\"pre\">1</span></code>. Only\n",
    "used if <code class=\"docutils literal notranslate\"><span class=\"pre\">penalty='elasticnet'</span></code>. Setting <code class=\"docutils literal notranslate\"><span class=\"pre\">l1_ratio=0</span></code> is equivalent\n",
    "to using <code class=\"docutils literal notranslate\"><span class=\"pre\">penalty='l2'</span></code>, while setting <code class=\"docutils literal notranslate\"><span class=\"pre\">l1_ratio=1</span></code> is equivalent\n",
    "to using <code class=\"docutils literal notranslate\"><span class=\"pre\">penalty='l1'</span></code>. For <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">&lt;</span> <span class=\"pre\">l1_ratio</span> <span class=\"pre\">&lt;1</span></code>, the penalty is a\n",
    "combination of L1 and L2.</p>\n",
    "</dd>\n",
    "</dl>\n",
    "</dd>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color =red>__That is a lot of parameters to choose from to fine tune our models__</font>   \n",
    "\n",
    "<font size=4> Lets just pick a couple from each and give them several different values.\n",
    "    \n",
    "__From PCA__\n",
    "    \n",
    "   n_components [.85,.9,.95] \n",
    "  \n",
    "    \n",
    "__From Logisitic Regression__\n",
    "    \n",
    "    C [1,2,3,4]\n",
    "    solver ['lbfgs','newton-cg']\n",
    "    \n",
    "    \n",
    "<font color=brown>  How many models is that total?  $ 3\\cdot 4 \\cdot 2$\n",
    "    \n",
    "    \n",
    "<font color=red> __It is very subtle but notice the double underscores in the code used in creating  the parameter grid. After kpca and log_reg__\n",
    "    \n",
    "    clf = Pipeline([(\"scale\",StandardScaler()),\n",
    "        (\"pca\", PCA()),\n",
    "        (\"log_reg\", LogisticRegression())\n",
    "        ])\n",
    "\n",
    "    param_grid = [{\n",
    "        \"pca__n_components\":[.85,.9,.95] ,\n",
    "        \"log_reg__C\":[1,2,3,4],\n",
    "        \"log_reg__solver\":['lbfgs','newton-cg']\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T15:39:08.811052Z",
     "start_time": "2022-02-03T15:39:08.790066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*4*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:21:01.927639Z",
     "start_time": "2023-02-02T17:21:01.798537Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "np.random.seed(5)\n",
    "data = datasets.load_digits()\n",
    "X=data.data\n",
    "y=data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>  Let's take a quick look at what [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) does before we introduce the pipeline.  In this Module Exercise we use PCA as a dimension reduction technique.  Our digits data set has 1797 samples and 64 features (8 $\\times$ 8 pixels). __Dimension reduction reduces the number of features of each sample.__ Let look at a PCA where we keep 90% of the variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:27:34.901773Z",
     "start_time": "2023-02-02T17:27:34.871159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797, 17))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=.85)#change to .99\n",
    "X_c=pca.fit_transform(X)\n",
    "X.shape,X_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>  Our reduced data set now has 21 features for each sample. Let's see how it performs with the Logistic Regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:28:14.400230Z",
     "start_time": "2023-02-02T17:28:06.357832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model {'log_reg__C': 1, 'log_reg__solver': 'lbfgs', 'pca__n_components': 0.9}\n",
      "Best Score 0.9154145798553145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "clf = Pipeline([ (\"pca\", PCA()),\n",
    "                (\"scale\",StandardScaler()),\n",
    "        (\"log_reg\", LogisticRegression())\n",
    "        ])\n",
    "\n",
    "param_grid = [{\n",
    "        \"pca__n_components\":[.85,.9,.95] ,\n",
    "        \"log_reg__C\":[1,2,3,4],\n",
    "        \"log_reg__solver\":['lbfgs','newton-cg']\n",
    "    }]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3,return_train_score=True,scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Model\",grid_search.best_params_)\n",
    "print(\"Best Score\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Let's look at all of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:28:28.261364Z",
     "start_time": "2023-02-02T17:28:28.215193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_pca__n_components</th>\n",
       "      <th>param_log_reg__C</th>\n",
       "      <th>param_log_reg__solver</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.915415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.915415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.913745</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.913745</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.912632</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.912632</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.912632</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.912632</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.95</td>\n",
       "      <td>3</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.912076</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.95</td>\n",
       "      <td>3</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.912076</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.95</td>\n",
       "      <td>4</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.911519</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.95</td>\n",
       "      <td>4</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.911519</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.910963</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.910963</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.910963</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.910963</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.9</td>\n",
       "      <td>4</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.910406</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.9</td>\n",
       "      <td>4</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.910406</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.908737</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.908737</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_pca__n_components param_log_reg__C param_log_reg__solver  \\\n",
       "1                      0.9                1                 lbfgs   \n",
       "4                      0.9                1             newton-cg   \n",
       "7                      0.9                2                 lbfgs   \n",
       "10                     0.9                2             newton-cg   \n",
       "11                    0.95                2             newton-cg   \n",
       "2                     0.95                1                 lbfgs   \n",
       "5                     0.95                1             newton-cg   \n",
       "8                     0.95                2                 lbfgs   \n",
       "17                    0.95                3             newton-cg   \n",
       "14                    0.95                3                 lbfgs   \n",
       "20                    0.95                4                 lbfgs   \n",
       "23                    0.95                4             newton-cg   \n",
       "13                     0.9                3                 lbfgs   \n",
       "16                     0.9                3             newton-cg   \n",
       "3                     0.85                1             newton-cg   \n",
       "0                     0.85                1                 lbfgs   \n",
       "19                     0.9                4                 lbfgs   \n",
       "22                     0.9                4             newton-cg   \n",
       "9                     0.85                2             newton-cg   \n",
       "6                     0.85                2                 lbfgs   \n",
       "\n",
       "    mean_test_score  rank_test_score  \n",
       "1          0.915415                1  \n",
       "4          0.915415                1  \n",
       "7          0.913745                3  \n",
       "10         0.913745                3  \n",
       "11         0.912632                5  \n",
       "2          0.912632                5  \n",
       "5          0.912632                5  \n",
       "8          0.912632                5  \n",
       "17         0.912076                9  \n",
       "14         0.912076                9  \n",
       "20         0.911519               11  \n",
       "23         0.911519               11  \n",
       "13         0.910963               13  \n",
       "16         0.910963               13  \n",
       "3          0.910963               15  \n",
       "0          0.910963               15  \n",
       "19         0.910406               17  \n",
       "22         0.910406               17  \n",
       "9          0.908737               19  \n",
       "6          0.908737               19  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF=pd.DataFrame(grid_search.cv_results_)\n",
    "DF=DF.sort_values(by=['rank_test_score'])\n",
    "Summary=DF[[ \"param_pca__n_components\",\n",
    "            \"param_log_reg__C\",\"param_log_reg__solver\",\"mean_test_score\",\"rank_test_score\"]]\n",
    "Summary.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> Exercise:  Compare this with out using PCA.  I had to bump my max_iter =500. Discuss results, i.e. did we lose much accuracy by doing PCA?  What did we gain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:32:08.808538Z",
     "start_time": "2023-02-02T17:32:04.901730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model {'log_reg__C': 1, 'log_reg__solver': 'lbfgs'}\n",
      "Best Score 0.9298831385642737\n"
     ]
    }
   ],
   "source": [
    "#clf = Pipeline([ (\"scale\",StandardScaler()),\n",
    "#        (\"log_reg\", LogisticRegression())])\n",
    "clf = Pipeline([ (\"scale\",StandardScaler()),\n",
    "       (\"log_reg\", LogisticRegression(max_iter=500))])\n",
    "\n",
    "param_grid = [{\"log_reg__C\":[1,2,3,4],\n",
    "        \"log_reg__solver\":['lbfgs','newton-cg']}]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3,return_train_score=True,scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Model\",grid_search.best_params_)\n",
    "print(\"Best Score\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> Exercise:  Does scaling really help?  Let's take it away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:34:09.564141Z",
     "start_time": "2023-02-02T17:33:06.839497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model {'log_reg__C': 2, 'log_reg__solver': 'lbfgs'}\n",
      "Best Score 0.9293266555370061\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([ \n",
    "       (\"log_reg\", LogisticRegression(max_iter=5000))])\n",
    "\n",
    "param_grid = [{\"log_reg__C\":[1,2,3,4],\n",
    "        \"log_reg__solver\":['lbfgs','newton-cg']}]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3,return_train_score=True,scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Model\",grid_search.best_params_)\n",
    "print(\"Best Score\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =5 color =red> __I don't like this one: Why?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Neural nets](https://en.wikipedia.org/wiki/Neural_network)\n",
    "\n",
    "<font size=4> \n",
    "    \n",
    "Let's look at Neural Nets and some of their parameters form [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).  Let's do a pipeline with a standard scaler and a Neural Net for the wine data set.  We will use \n",
    "    \n",
    "    param_grid = [{\n",
    "        \"mlp_clf__hidden_layer_sizes\":[(16,16,16,16),(256,256,256,256)],\n",
    "        \"mlp_clf__solver\":['lbfgs','sgd','adam'],\n",
    "        \"mlp_clf__activation\":['identity', 'logistic', 'tanh', 'relu']\n",
    "        }]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:45:46.548788Z",
     "start_time": "2023-02-02T17:45:46.518095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "wine = datasets.load_wine()\n",
    "features=np.array(wine.feature_names)\n",
    "features\n",
    "X=wine.data\n",
    "y=wine.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:46:43.262110Z",
     "start_time": "2023-02-02T17:46:43.240125Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "clf = Pipeline([ (\"scale\",StandardScaler()),(\"mlp_clf\", MLPClassifier(max_iter=1000))])\n",
    "\n",
    "\n",
    "param_grid = [{\n",
    "        \"mlp_clf__hidden_layer_sizes\":[(16,16,16,16),(256,256,256,256)],\n",
    "        \"mlp_clf__solver\":['lbfgs','sgd','adam'],\n",
    "        \"mlp_clf__activation\":['identity', 'logistic', 'tanh', 'relu']\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:48:17.326126Z",
     "start_time": "2023-02-02T17:47:02.965590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model {'mlp_clf__activation': 'relu', 'mlp_clf__hidden_layer_sizes': (256, 256, 256, 256), 'mlp_clf__solver': 'sgd'}\n",
      "Best Score 0.9833333333333334\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(clf, param_grid, cv=4,return_train_score=True,scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Model\",grid_search.best_params_)\n",
    "print(\"Best Score\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =4>  Let's look at the shape of the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T18:18:04.743926Z",
     "start_time": "2023-02-02T18:18:04.729912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13, 256), (256, 256))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.steps[1][1].coefs_\n",
    "M1=grid_search.best_estimator_.named_steps['mlp_clf'].coefs_[0]\n",
    "M2=grid_search.best_estimator_.named_steps['mlp_clf'].coefs_[1]\n",
    "np.shape(M1), np.shape(M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T17:49:12.801990Z",
     "start_time": "2023-02-02T17:49:12.773667Z"
    }
   },
   "outputs": [],
   "source": [
    "DF=pd.DataFrame(grid_search.cv_results_)\n",
    "#DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T16:58:43.997101Z",
     "start_time": "2022-02-03T16:58:43.975115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_mlp_clf__activation</th>\n",
       "      <th>param_mlp_clf__hidden_layer_sizes</th>\n",
       "      <th>param_mlp_clf__solver</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>relu</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tanh</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.983207</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>logistic</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.977652</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tanh</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.977652</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>relu</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identity</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tanh</td>\n",
       "      <td>(16, 16, 16, 16)</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>relu</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic</td>\n",
       "      <td>(16, 16, 16, 16)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity</td>\n",
       "      <td>(256, 256, 256, 256)</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_mlp_clf__activation param_mlp_clf__hidden_layer_sizes  \\\n",
       "22                      relu              (256, 256, 256, 256)   \n",
       "17                      tanh              (256, 256, 256, 256)   \n",
       "11                  logistic              (256, 256, 256, 256)   \n",
       "16                      tanh              (256, 256, 256, 256)   \n",
       "21                      relu              (256, 256, 256, 256)   \n",
       "4                   identity              (256, 256, 256, 256)   \n",
       "13                      tanh                  (16, 16, 16, 16)   \n",
       "23                      relu              (256, 256, 256, 256)   \n",
       "6                   logistic                  (16, 16, 16, 16)   \n",
       "5                   identity              (256, 256, 256, 256)   \n",
       "\n",
       "   param_mlp_clf__solver  mean_test_score  rank_test_score  \n",
       "22                   sgd         0.983333                1  \n",
       "17                  adam         0.983207                2  \n",
       "11                  adam         0.977652                3  \n",
       "16                   sgd         0.977652                3  \n",
       "21                 lbfgs         0.972222                5  \n",
       "4                    sgd         0.972096                6  \n",
       "13                   sgd         0.972096                6  \n",
       "23                  adam         0.972096                8  \n",
       "6                  lbfgs         0.972096                8  \n",
       "5                   adam         0.972096                8  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF=DF.sort_values(by=['rank_test_score'])\n",
    "Summary=DF[[\"param_mlp_clf__activation\",\"param_mlp_clf__hidden_layer_sizes\",\"param_mlp_clf__solver\",\"mean_test_score\",\"rank_test_score\"]]\n",
    "Summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> Exercise:  Compare a Neural Net and a Logistic regression on the wine data set in a pipeline.  Use all the same parameters above for both the MLP and the Logisitc regression. Do not do any PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
